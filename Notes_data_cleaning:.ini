Complete Data Cleaning Summary — From Start to Finish

Starting Point
Raw dataset:    23,118 image pairs
Each pair:      distorted image + corrected image
Goal:           Build clean labeled dataset for
                k1/k2 regression model

Step 1: Basic Structure Analysis
What We Did
Checked dimensions, orientations, and whether each pair had matching sizes.
What We Found
All pairs same size:     ✓ confirmed
Width almost always:     2048px (fixed)
Height varies:           1342 → 1534px
Aspect ratio:            ~1.499 (nearly constant)
What It Means
Company pipeline:
  Apply undistortion → black borders appear
  Resize corrected image BACK to original dimensions
  Output = same size as input

This means our inference pipeline is:
  predict k1/k2 → cv2.undistort → cv2.resize(w,h) → done
Why It Matters
If we got this wrong and submitted images at wrong size
the scoring metric would penalize every single pixel.
Getting this right is foundational to everything else.

Step 2: Orientation Discovery
What We Found
Landscape images:    22,867  (98.9%)
Portrait images:        251   (1.1%)
What It Means
251 images were shot with camera rotated 90 degrees.
Height > Width for these.
Decision
KEEP portrait images.
Rotate to landscape before feeding to model.
Barrel distortion physics are the same regardless of orientation.
Same k1/k2 applies after rotation.
Rotating back after prediction gives correct output.

Why keep them:
  Real business scenario — photographers do rotate cameras
  Model should handle both orientations
  Adding them makes model more robust

Step 3: Height Group Discovery
What We Found
Main group  (h=1342-1380):  22,833 images  (98.8%)
Tall group  (h=1515-1534):     284 images   (1.2%)

These are TWO distinct groups.
Heights between 1380-1515 simply do not exist.
What It Means
Tall group = different camera sensor (different aspect ratio)
2048/1366 = 1.499 (main group)
2048/1534 = 1.335 (tall group)
Investigation
Ran visual comparison:
  Main group (h=1366, diff=12.4) vs
  Tall group (h=1534, diff=12.7)

Both showed:
  Same barrel distortion pattern visually
  Same diff map structure (edges bright, center dark)
  Same radial correction profile (low center, high edges)
  
Conclusion: SAME distortion type, different camera body.
Decision
KEEP tall group.
Same k1/k2 physics applies.
Camera matrix is always built from actual image dimensions.
One model handles both automatically.

Why not separate:
  284 images not enough to train a separate model
  Distortion looks identical to main group
  Being camera-agnostic is the whole point of the model

Step 4: Center Pixel Analysis
What We Did
Measured pixel difference at exact image center vs edges.
What We Found
Single center pixel mean diff:   0.9054
Median:                          1.0000
89.3% of images below 2.0 at center

Files confirmed as JPEG format.
What It Means
For pure barrel distortion:
  Center pixel should NOT move at all
  Any center diff = noise, not geometry

Center diff ≈ 1.0 = pure JPEG compression noise
JPEG compression introduces ~1-5 pixel noise everywhere
Including pixels that geometrically did not change

Conclusion: Dataset has PURE barrel distortion.
The geometric correction is clean.
k1/k2 model is exactly the right approach.
Why It Matters
If center diff was high (>5) AND files were PNG (lossless):
  Would mean company applied color/exposure changes too
  k1/k2 would only explain part of the correction
  Model would always have irreducible error

Confirmed it is just JPEG noise → irreducible but harmless.

Step 5: Distortion Magnitude Categorization
What We Did
Computed overall mean pixel difference for every pair.
What We Found
Min overall diff:    0.000  ← some pairs identical
Max overall diff:   30.710  ← heavy distortion
Mean overall diff:   6.769
Distribution:        right-skewed, most images 2-12
Four Categories Created
Category 1: Identical (diff < 0.5)
Count:    134 images (0.6%)
Meaning:  Distorted and corrected are essentially the same image
          No correction was applied OR lens had zero distortion

Why remove:
  These images would teach the model:
  "This visual appearance → k1 = 0.0"
  But similar-looking images in dataset have k1 = -0.15
  Direct contradiction in training labels
  Model gets confused, predictions pulled toward zero
  
Decision: REMOVE entirely
Category 2: Mild (diff 0.5-2.0)
Count:    699 images (3.0%)
Meaning:  Very small lens distortion
          Geometric change (~1px) ≈ JPEG noise (~1px)
          Cannot reliably separate signal from noise

Why keep with low weight:
  Real lenses DO have mild distortion
  Test set will have mild cases too
  Removing creates blind spot in model
  
Why low weight (0.3):
  k1/k2 labels extracted from these will be noisy
  JPEG noise dominates the geometric signal
  Model should learn from these but not trust them fully
  
Decision: KEEP with weight=0.3
Category 3: Normal (diff 2.0-20.0)
Count:  22,138 images (95.8%)
Meaning: Standard barrel distortion
         Clear geometric signal well above JPEG noise
         k1/k2 extraction will be reliable

Decision: KEEP with weight=1.0 (standard)
Category 4: Heavy (diff > 20.0)
Count:    147 images (0.6%)
Meaning:  Strong barrel distortion (wide angle lenses)
          Large geometric change, very clear signal
          k1/k2 extraction will be most accurate here

Why high weight (5.0):
  Only 147 images vs 22,138 normal = 150x underrepresented
  Without boosting, model sees these 150x less
  Model would never properly learn heavy correction
  But these are the MOST IMPORTANT cases:
    Wide angle lenses distort the most
    Hardest to correct visually
    Most noticeable when wrong
    Directly affects competition edge metric (40%)
    
Decision: KEEP with weight=5.0, oversample with augmentation

Step 6: Suspicious Barrel Ratio Discovery
What We Did
For each image computed:
  corner_diff = mean pixel diff in image corners
  center_diff = mean pixel diff at image center
  barrel_ratio = corner_diff / center_diff

Pure barrel: corners change MUCH more than center
barrel_ratio should be >> 1.0 (ideally 3-10x)
What We Found
Suspicious (ratio < 1.2):  2,386 images (10.3%)

These images had:
  Corner diff ≈ center diff
  Correction happened uniformly everywhere
  NOT the expected barrel pattern
Investigation — Diff Maps
Examined suspicious images visually.

Diff maps showed:
  NOT uniform brightness (not color change)
  NOT U-shaped profile (not barrel)
  Instead: BRIGHT OUTLINES around every object edge
  
  Wall edges glow. Furniture edges glow.
  Flat regions completely dark.
  
  This is the signature of IMAGE TRANSLATION.
  When you shift an image by N pixels:
    Every edge becomes a bright line in diff
    Every flat region cancels to zero

Step 7: Shift Detection and Removal
What We Did
Applied phase correlation (cv2.phaseCorrelate) to
all 2,386 suspicious images.

Phase correlation finds the global translation (dx, dy)
between two images in the frequency domain.
Fast and accurate for detecting pure shifts.
What We Found
Actually shifted (>2px):   651 images  (27.3% of suspicious)
Not shifted (<2px):      1,735 images  (72.7% of suspicious)

Max shift:   130.4 pixels
Mean shift:    2.9 pixels
Median shift:  0.3 pixels (most have NO shift)

Direction: random (both dx and dy, positive and negative)
Correlation with diff: none (shift is pipeline artifact, not distortion)
Property Level Pattern
Unique properties:              1,657
Properties ALL shifted:           319  → different pipeline batch
Properties SOME shifted:          180  → mixed within property
Properties NOT shifted:         1,158  → clean

Interpretation:
  319 properties processed by different pipeline version
  180 properties had mixed lens types mid-shoot
  Shift is per-session/pipeline, not per-lens
Why Shifts Destroy k1/k2 Labels
k1/k2 extraction works by:
  Finding matching points in distorted and corrected
  Fitting radial model to explain their displacement

If image is also shifted 25px left:
  Every point appears to move: barrel warp + 25px left
  Optimizer tries to fit radial model to this combined motion
  Gets completely wrong k1/k2 values
  Wrong label → model learns wrong relationship
  
A wrong label is worse than no label.
Decision
REMOVE all 651 shifted images regardless of category.
Shift overrides all other considerations.

Why not try to correct the shift:
  Shift magnitude varies 2-130px (not consistent)
  Phase correlation not 100% reliable for large shifts
  Some shifts may be diagonal not just horizontal
  22,333 clean images is already enough
  Complexity not worth it
  
Decision: REMOVE shifted, keep non-shifted

Final Dataset After All Cleaning
STARTED WITH:                   23,118 pairs
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
REMOVED:
  Step 5 — Identical pairs:        134  (wrong labels)
  Step 7 — Shifted images:         651  (corrupted geometry)
  Total removed:                   785  (3.4%)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
TRAINING SET:                   22,333 pairs

  Mild    (weight=0.3):            699  (3.1%)
  Normal  (weight=1.0):         21,487  (96.2%)
  Heavy   (weight=5.0):            147  (0.7%)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

What Each Cleaning Step Protected Against
Step              What it prevented
──────────────────────────────────────────────────────────────
Remove identical  Model learning k1=0 for non-zero distortion
                  Contradictions in training labels
                  Predictions pulled toward zero

Remove shifted    Wrong k1/k2 labels from shift-contaminated pairs
                  Model learning barrel correction from translation
                  Systematically biased predictions

Down-weight mild  Overfitting to noisy labels
                  JPEG noise mistaken for geometric signal
                  Model confused about mild distortion regime

Up-weight heavy   Model ignoring rare but critical cases
                  Poor correction on wide-angle lenses
                  Low edge similarity score on heavy barrel images

Keep portraits    Missing 251 valid training examples
                  Model failing on portrait test images

Keep tall group   Missing 284 valid training examples
                  Model failing on different camera body images
                  Failing to generalize to new camera+lens combos